---
title: {{TITLE}}
theme: "comp421"
separator: "^---"
verticalSeparator: "^----"
notesSeparator: "^Note:"
revealOptions:
    transition: fade

---

### Chapter 15:  Query Processing

- By the end of class you should know
  - Measures of Query Cost
  - Selection Operation 
  - Sorting

Next class we will cover join operations

Note:

https://www.db-book.com/slides-dir/PDF-dir/ch15.pdf

---

### Basic Steps in Query Processing

1. Parsing and translation
2. Optimization
3. Evaluation

<figure>
 <img src="{{HTTPS_FQDN}}/static/images/Chapter.15.Figure.15.1.Steps-in-query-processing.png" style="width:50%"/>
 <figcaption>Figure 15.1 Steps in query processing</figcaption> 
</figure>

Note:

At a high level, these are the steps in the processing of a query.

---

### Basic Steps in Query Processing (Cont.)

- Parsing and translation
  - Translate the query into its internal form
  - Translated internal form into relational algebra
  - Parser checks syntax, verifies relations
- Evaluation
  - The query-execution engine takes a query-evaluation plan
  - Executes that plan
  - Returns the answers to the query.

Note:

The Evaluation creates and takes a plan. **some plan**  

It doesn't have to pick the absolute best plan.  Just a very good one.

---

### Basic Steps in Query Processing:
#### Optimization

- A relational algebra expression may have many equivalent expressions
  - $\sigma_{\text{salary} \lt 75000} ( \Pi_{\text{salary}}(\text{instructor}))$ is equivalent to $\Pi_{\text{salary}}(\sigma_{\text{salary} \lt 75000} (\text{instructor}))$
- Each algebra operation can be evaluated in one of many ways
- **evaluation-plan** used detailed expression evaluation strategy
  - Use index on <i>salary</i> to find instructors with salary &lt; 75000
  - Or scan and discard instructors with salary $\lt$ 75000
  - Which would you pick?

Note:

Ask the students which they would pick


---

### Basic Steps:
#### Optimization (Cont.)

- Amongst all equivalent evaluation plans choose one with lowest cost
  - Cost estimated using statistical information from database catalog
    - e.g. number of tuples in each relation, size of tuples, etc.
- In this chapter we study
  - How to measure query costs
  - Algorithms for evaluating relational algebra operations
  - How to combine algorithms for individual operations in order to evaluate a complete expression


---

### Measures of Query Cost

- Many factors contribute to time cost
  - Disk access, CPU, network communications
- Cost can be measured based on 
  - **response time** total elapsed time or
  - total **resource consumption**
- We use total resource consumption as cost metric
  - Response time harder to estimate
- We ignore CPU costs for simplicity
  - Real systems do take CPU cost into account
  - Network costs must be considered for parallel systems
- We describe how estimate the cost of each operation
  - We do not include cost to writing output to disk
  - Why?

Note:

Do not compute the cost to write to disk because sometimes you
don't have to write to disk for intermediate computations or for selects

---

### Measures of Query Cost

- Disk cost can be estimated as:
  - Number of seeks times average-seek-cost
  - Number of blocks read times average-block-read-cost
  - Number of blocks written times average-block-write-cost
- For simplicity we just use the **number of block transfers** from disk and the **number of seeks** as the cost measures
  - $t_T$ – time to transfer one block
    - Assuming read cost equals write cost
  - $t_S$ – time for one seek
  - Cost for $b$ block transfers plus $S$ seeks $b * t_T + S * t_S$
- $t_S$ and $t_T$  depend on where data is stored; with 4 KB blocks:
  - High end magnetic disk: $t_S = 4$ msec &AMP; $t_T =0.1$ msec
  - SSD: : $t_S = 20-90$ $\mu$sec &AMP; $t_T = 2-10$ $\mu$sec for 4KB 

Note:
 
1000 $\mu$sec = 1 msec

---

### Measures of Query Cost (Cont.)

- Required data may be in memory already, avoiding  I/O
  - But hard to take into account for cost estimation
- Several algorithms reduce disk IO by using extra buffers 
  - Amount of real memory available depends on other concurrent queries and OS processes
    - Known only during execution
- Worst case estimates assume no data is initially in buffer / minimum buffers
  - But more optimistic estimates are used in practice

Note: 

Lots of possibilities exist, but we'll make some simplifications

---

### Selection Operation
#### File scan

- Scan each file's block; test all records against selection condition
  - Cost estimate = $b_r$ block transfers + 1 seek
    - $b_r$ denotes number of blocks in relation $r$
  - If selection on primary key, stop on finding record
    - cost = $(b_r / 2) $ block transfers + 1 seek
  - Linear search can be applied regardless of
    - selection condition or
    - ordering of records in the file, or 
    - availability of indices
- Binary search generally does not make sense
  - Except when there is an index available
  - Index search faster than binary search


---

### Selections Operation
#### Using Indices

- **clustering index, equality on key** Retrieve a single record satisfies condition
  - Cost = $(h_i+1) * (t_T + t_S)$ index on disk
  - Cost = $t_T + t_S$ if index in memory
- **clustering index, equality on non primary key** Retrieve multiple records
  - Records will be on consecutive blocks
    - Let $b$ = number of blocks containing matching records
  - Cost = $h_i * (t_T + t_S) + t_S + b * t_T $ index on disk
  - Cost = $t_S + b * t_T $ index in memory

Note:

$h_i$ is the height of the index  

Clustering index assumes you have to read through, possibly, the entire index and then read one block.  
Simplified if index in memory.  
<font color='red'>What if only the search keys are retrieved??</font>

---

### Selections Operation
#### Using Indices

- **secondary index, equality on key/non-key** 
  - Retrieve a single record if search-key is candidate key
    - Cost = $(h_i + 1) * (t_T + t_S)$ index on disk
    - Cost = $t_T + t_S$ index in memory
  - Retrieve multiple records if search-key is not a candidate key
    - each of $n$ matching records may be on a different block  
    - Cost =  $(h_i + n) * (t_T + t_S)$ index on disk
    - Cost = $n * (t_T + t_S)$ index in memory
  - Can be very expensive!

Note:

Cost of retrieving a single record read the height of the tree, $h_i$ costing ($t_T + t_S$) for each index node. Then it costs on $t_T + t_S$ for the actual record.

If there are $n$ matching records it's $h_i +n$ costing $t_T + t_S$.

---

### Selections Involving Comparisons

- $\sigma_{\text{a} \le v} (r)$ or $\sigma_{\text{a} \ge v}(r)$ processed: (Relation sorted on A)
  - a linear file scan,
  - or by indices in the following ways:
- **clustering index, comparison** (Index on A)
    - For $\sigma_{\text{a} \ge v}(r)$ use index find first tuple and scan from there
    - For $\sigma_{\text{a} \le v} (r)$ just scan until until tuple $\lt v$
- **secondary index, comparison**
    - Same as sorted on A, but instead scan **index** 
    - Non-clustered records are pointed to by index
    - An extra I/O per record
    - Linear file scan may be cheaper!

Note:

Assume clustering index in the primary key.


Secondary (non-clustering) index requires $(h_i + n) * (t_T + t_S)$.
Probably cheaper to just scan the file

---

### Complex Selections
#### Conjunction

- **Conjunction**: $\sigma_{\theta_1 \land \theta_2 \land \dots \land \theta_n}(r)$
  - Use multiple-key index if appropriate
  - Do intersection of each conjunctive selection
    - Requires indices with record pointers
    - After processing each condition, fetch record from file
    - If conditions don't have indices, read record &amp; apply test in memory

Note:

Each conjunction requires done separately.
But don't fetch the records until doing the conjunction.  
Then when you do the AND's on the pointers, before retrieving data

---

### for Complex Selections
#### Disjunction

- **Disjunction**: $\sigma_{\theta_1 \vee \theta_2 \vee \dots \vee \theta_n}(r)$
  - Union of pointers of all conditions have available indices
    - Otherwise use linear scan.
  - Use corresponding index for $\theta$, take union pointers
  - Then fetch records from file
- **Negation**: $\sigma_{\neg \theta}(r)$
  - Use linear scan on file
  - If very few records satisfy $\neg \theta$, and index on $\theta$
    - Find satisfying records using index and fetch from file

Note:

Similar to conjunction in that do the logical or of the record pointers.

Negations generally are a scan assuming few records match

---

{{HIDDEN}} 

### Bitmap Index Scan

- The <b>bitmap index scan </b>algorithm of PostgreSQL
  - Bridges gap between secondary index scan and linear file scan when number of matching records is not known before execution
  - Bitmap with 1 bit per page in relation
  - Steps:
    - Index scan used to find record ids, and set bit of corresponding page in bitmap
    - Linear file scan fetching only pages with bit set to 1
  - Performance
    - Similar to index scan when only a few bits are set
    - Similar to linear file scan when most bits are set
    - Never behaves very badly compared to best alternative


---

### Sorting Options

1. Build a new index on relation
   - Use temp index to read the relation in sorted order
   - May lead to one disk block access for each tuple.
2. For relations that fit in memory, quicksort can be used.  
3. For relations that don’t fit memory, **external sort-merge**

Note:

there are three options for sorting

---

### External Sorting Using Sort-Merge

- Assume only 1 record fit within a buffer; 3 buffers
<figure>
 <img src="{{HTTPS_FQDN}}/static/images/Chapter.15.Figure.15.4.External-sorting-using-sort-merge.png" style="width:40%"/>
 <figcaption>Figure 15.4 External sorting using sort-merge</figcaption> 
</figure>

Note:

Walk through that in the first pass we could only fit the first three records

---

### External Sort-Merge

- Let $M$ denote memory size in pages 
  1. **Create sorted runs**. 
     * Let $i=0$ initially  
     * Repeatedly do the following till the end of the relation:  
       - Read $M$ blocks of relation into memory  
       - Sort the in-memory blocks  
       - Write the sorted data to run $R_i$, increment $i$  
     - Let $N = i$  
  2.Merge the runs (next slide)

Note:

The first run has only three records sorted because 3 buffers, then the next three records are sorted.

merge pass 1 takes the first two runs and creates runs of six pages.

---

### External Sort-Merge (Cont.)

2. **Merge the runs (N-way)** First assume $N \lt M$
   - Use $N$ blocks memory for input, 1 block output
     - Read the first block of each run into its buffer
   - Repeat
     - Select the sorted-order first record in $N$ buffers
     - Move selected record to output buffer. (Write if full)
     - Delete the record from its input buffer page.
       - If input buffer empty, read next block
   - until all input buffer pages are empty

Note:

We can first assume that there are N way where N is less than M,

---

### External Sort-Merge (Cont.)

- If $N \ge M$, more merge passes required
  - Each pass, $M-1$ runs are merged
  - Each new run increased by $M-1$ factor
    - If $M=11$, and there are 90 runs, one pass reduces the number of runs to 9, each 10 times the size of the initial runs
  - Repeated passes are performed till all runs have been merged into one


---

### External Merge Sort (Cont.)

- Cost analysis:
  - 1 block/run $\rightarrow$ **too many seeks**
    - Instead use $b_b$ buffer blocks per run
      - read/write $b_b$ blocks at a time
    - Can merge $\lfloor M/b_b \rfloor -1$ runs in one pass
  - Total number of merge passes required: $\lceil \log_{ \lfloor M / b_b \rfloor -1 } (b_r/M) \rceil$
  - Block transfers for each run is $2 b_r$
    - But for final pass, we don’t write
    - Total # block transfers for external sorting:<br/> $b_r ( 2 \lceil \log_{\lfloor M / b_b \rfloor +1 } (b_r/M) \rceil + 1)$
  - Seeks: next slide

Note:

$b_r$ is number of blocks in relation  
$b_b$ is number of blocks read in at a time 

The number of block transfers is not the number of I/O.  This is the number of transfers

For Figure 15.4, read in/out each run 24 I/Os but only 12 for the last run  
12 * (2 + 2+ 1) = 60

---

### External Merge Sort (Cont.)

- Cost of seeks
  - During run generation: one seek to read each run / one to write each run <br />$2 \lceil b_r / M \rceil$
  - During the merge phase
    - **Estimate** $2 \lceil b_r / b_b \rceil$ seeks for each merge pass except final pass
    - Total number of seeks $2 \lceil b_r / M \rceil + \lceil b_r/ b_b \rceil ( 2 \lceil \log_{M/b_b -1} (b_r/M) \rceil -1 )$


Note:

$b_r$ is number blocks in relation  
$b_b$ is the number of buffers  

Number runs is number blocks in relation $b_r$ divided by number buffers $M$ or $b_r / M$

$2 b_r$ I/O for each pass  
The initial run is $\lceil b_r / M \rceil$ seeks  

Each pass read does $\lceil b_r / b_b \rceil$  
Each pass does same number of writes except for last one  

There are $\lceil \log_{\lceil M / b_b \rceil -1}(b_r/M) \rceil$ runs

 
{{HTTPS_FQDN}}/pages/worksheets/worksheet-15-A-external-merge-sort
{{HTTPS_FQDN}}/worksheets/worksheet-15-A-external-merge-sort

---

### Epilogue 

- Database performance requires using the correct algorithm
- Largest cost is I/O; At a high level CPU can be ignored
- Selection can scan, use indices, involve comparisons
  - Conjunction, disjunction, and negation all play a role
- Sorting complicated
  - quick-sort only used on the smallest of relations
  - External merge-sort key to limiting I/O

- Next class joins

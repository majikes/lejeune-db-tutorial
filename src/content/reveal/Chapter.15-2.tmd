---
title: {{TITLE}}
theme: "comp421"
separator: "^---"
verticalSeparator: "^----"
notesSeparator: "^Note:"
revealOptions:
    transition: fade

---

### Chapter 15:  Query Processing

- By the end of class you should know
  - Join Operation

Note:

https://www.db-book.com/slides-dir/PDF-dir/ch15.pdf

---

### Join Operation

- Several different algorithms to implement joins
  1. Nested-loop join
  2. Block nested-loop join
  3. Indexed nested-loop join
  4. Merge-join
  5. Hash-join
- Choice based on cost estimate
- Examples use the following information
  - 5,000 student records; 100 student blocks
  - 10,000 takes records; 400 student blocks


---

### Nested-Loop Join

- To compute the theta join $r \bowtie_\theta s$
  - for each tuple $t_r$ in $r$ do begin
    - for each tuple $t_s$ in $s$ do begin
      - test $\theta_{t_r, t_s}$
        - if true add $t_r, t_s$ to output
      - end
   - end 
- $r$ is the outer relation; $s$ is the inner relation
- Requires no indices; can be used with any join condition
- Expensive; it examines every pair of tuples in two relations


---

### Nested-Loop Join (Cont.)

- In the worst case, if there is enough memory only to hold one block of each relation, the estimated cost is $n_r * b_s + b_r$ block transfers plus $n_r + b_r$ seeks
- If one relation fits in memory, use smaller as inner relation
  - Reduces cost to $b_r + b_s$ block transfers and 2 seeks
- Assuming worst case memory availability cost estimate is
  - with _student_ as outer relation:
    - 5000 ∗ 400 + 100 = 2,000,100 block transfers,
    - 5000 + 100 = 5100 seeks 
  - with _takes_ as the outer relation 
    - 10000 ∗ 100 + 400 = 1,000,400 block transfers and 10,400 seeks
- If student fits entirely in memory, the cost estimate will be 500 block transfers.
- Block nested-loops algorithm (next slide) is preferable.

Note:

Number tuples in $r$ is $n_r$. $s$ has $n_s$

Number of buffers for $r$ is $b_r$, $s$ uses $b_s$

https://comp421.cs.unc.edu/pages/worksheets/worksheet-15-B-nested-loop

https://comp421.cs.unc.edu/worksheets/worksheet-15-B-nested-loop
---

### Block Nested-Loop Join

- Variant of nested-loop: every block of inner relation paired with every block of outer relation
  - for each Block $B_r$ of $r$ do begin
    - for each block $B_s$ of $s$ do begin
      - for each tuple $t_r$ in $B_r$ do begin
        - for each tuple $t_s$ is $B_s$ do begin
           - test $\theta(t_r, t_s)$
             - if true add $t_r, t_s$ to output
  - end // Not all end statements shown

- Cost: $b_r * b_s + b_r$ block transfers
  - Which relation is outer versus inner loop?


Note:

Make the outer loop $b_r$ it saves $b_s - b_r$ block transfers

---

### Indexed Nested-Loop Join

- Index lookups can replace file scans if
  - join is an equi-join or natural join and
  - Index is available on **inner relation’s** join attribute
    - Can construct temp index just to compute a join.
- For each tuple $t_r$ in the outer relation $r$ use index for $t_s$ lookup that satisfy $\theta(t_r, t_s)$
- Worst case:  buffer has space for only one page of $r$, and, for each tuple in $r$, perform an index lookup on $s$
- Cost of the join:  $b_r(t_T + t_s) + n_r * c$
  - Where $c$ is cost of index and fetch of matching $s$ tuples
  - $c$ can be estimated as cost of a single selection on $s$ using $\theta$
- If indices are available on join attributes of both $r$ and $s$ use the relation with fewer tuples as the outer relation


---

### Merge-Join

- Sort both relations on their join attribute (if not already sorted on the join attributes)
- Merge the sorted relations to join them
  - Join step is similar to the merge stage of the sort-merge algorithm.
  - Main difference is handling of duplicate values in join attribute — every pair with same value on join attribute must be matched
- Detailed algorithm in book

<figure>
 <img src="{{HTTPS_FQDN}}/static/images/Chapter.15.Figure.15.8.Sorted-relations-for-merge-join.png" style="width:20%"/>
 <figcaption>Figure 15.8 Sorted relations for merge join</figcaption> 
</figure>

---

### Merge-Join (Cont.)

- Can be used only for equi-joins and natural joins
- Each block needs to be read only once assuming all tuples of any join fit in memory
- Thus the cost of merge join is: $b_r + b_s$ block transfers + $\lceil b_r/b_b \rceil + \lceil b_s/ b_b \lceil$ seeks
  - plus sorting cost if relations are unsorted

Note:

Removed hybrid join
- **hybrid merge-join**: If one relation is sorted, and the other has a secondary B$^+$-tree index on the join attribute
  - Merge the sorted relation with the leaf entries of the B$^+$-tree
  - Sort the result on the addresses of the unsorted relation’s tuples
  - Scan the unsorted relation in physical address order and merge with previous result, to replace addresses by the actual tuples
    - Sequential scan more efficient than random lookup


---

### Hash-Join

- Applicable for equi-joins and natural joins.
- A hash function<i> h</i> is used to partition tuples of both relations 
- $h$ maps _JoinAttrs_ values to {0, 1, ..., <i>n</i>}, where _JoinAttrs_ denotes the common attributes of $r$ and $s$ used in the natural join
  - $r_0, r_1, \dots , r_n$ denote partitions of $r$ tuples
    - Each tuple $t_r \in r$ is put in partition $r_i$ where $i = h(t_r[\text{JoinAttrs}])$
  - $s_0, s_1, \dots , s_n$ denote partitions of $s$ tuples
    - Each tuple $t_s \in s$ is put in partition $s_i$ where $i = h(t_s[\text{JoinAttrs}])$

---

### Hash-Join (Con't)

<figure>
 <img src="{{HTTPS_FQDN}}/static/images/Chapter.15.Figure.15.9.Hash-partitioning-of-relations.png" style="width:50%"/>
 <figcaption>Figure 15.9 Hash partitioning of relations</figcaption> 
</figure>

---

{{HIDDEN}}

### Hash-Join Algorithm

- Partition the relation $s$ using hashing function $h$.  When partitioning a relation, one block of memory is reserved as the output buffer for each partition.
- Partition $r$ similarly
- For each $i$: 
  - Load $s_i$ into memory and build an in-memory hash index on it using the join attribute.  This hash index uses a different hash function than the earlier one $h$
  - Read the tuples in $r_i$ from the disk one by one.  For each tuple $t_r$ locate each matching tuple $t_s$ in $s_i$ using the in-memory hash index.  Output the concatenation of their attributes.  The hash-join of $r$ and $s$ is computed as follows.  Relation $s$ is called the **build input** and $r$ is called the **probe input**

---

{{HIDDEN}}

### Hash-Join Algorithm

- The value $n$ and the hash function $h$ is chosen such that each $s_i$ should fit in memory.
  - Typically n is chosen as $\lceil b_s / M \rceil * f$ where $f$ is a “<b>fudge factor</b>”, typically around 1.2
  - The probe relation partitions $s_i$ need not fit in memory
- <b>Recursive partitioning</b> required if number of partitions $n$ is greater than number of pages $M$ of memory.
  - instead of partitioning <i>n</i> ways, use<i> M –</i> 1 partitions for s
  - Further partition the <i>M –</i> 1 partitions using a different hash function
  - Use same partitioning method on $r$
  - Rarely required: e.g., with block size of 4 KB, recursive partitioning not needed for relations of &lt; 1GB with memory size of 2MB, or relations of &lt; 36 GB with memory of 12 MB


---

{{HIDDEN}}

### Handling of Overflows

- Partitioning is said to be <b>skewed</b> if some partitions have significantly more tuples than some others
- <b>Hash-table overflow</b> occurs in partition $s_i$ if $s_i$ does not fit in memory.  
<p>Reasons could be/p>
  - Many tuples in s with same value for join attributes
  - Bad hash function
- <b>Overflow resolution</b> can be done in build phase
  - Partition $s_i$ is further partitioned using different hash function. 
  - Partition $r_i$ must be similarly partitioned.
- <b>Overflow avoidance</b> performs partitioning carefully to avoid overflows during build phase
  - E.g. partition build relation into many partitions, then combine them
- Both approaches fail with large numbers of duplicates
  - Fallback option: use block nested loops join on overflowed  partitions


---

{{HIDDEN}}

### Cost of Hash-Join

- If recursive partitioning is not required: cost of hash join is
<p>3(<i>b</i><i>r</i> +<i> b</i><i>s</i><i>)</i> +4 ∗<i> n</i><i>h  </i>block transfers +</p>
<p>2( <i>b</i><i>r </i><i>/ b</i><i>b</i> + <i>b</i><i>s </i><i>/ b</i><i>b</i>)  seeks</p>
- If recursive partitioning required:
  - number of passes required for partitioning build relation <i>s </i>to less than M blocks per partition is <i>log</i><i>M/bb</i>–1(<i>b</i><i>s</i><i>/M</i>)
  - best to choose the smaller relation as the build relation.
  - Total cost estimate is: 
<p>2<i>(b</i><i>r</i><i> + b</i><i>s</i>) <i>log</i><i>M/bb</i>–1(<i>b</i><i>s</i><i>/M</i>) + <i>b</i><i>r</i><i> + b</i><i>s  </i>block transfers + </p>
<p>2(<i>b</i><i>r </i><i>/ b</i><i>b</i> + <i>b</i><i>s </i><i>/ b</i><i>b</i>) <i>log</i><i>M/bb</i>–1(<i>b</i><i>s</i><i>/M</i>)  seeks</p>
- If the entire build input can be kept in main memory no partitioning is required
  - Cost estimate goes down to <i>b</i><i>r</i><i> + b</i><i>s</i>


---

{{HIDDEN}}


### Hybrid Hash-Join

- Useful when memory sized are relatively large, and the build input is bigger than memory.
- <b>Main feature of hybrid hash join:</b> <b>Keep the first partition of the build relation in memory.</b>
- E.g. With memory size of 25 blocks, <i>instructor </i>can be partitioned into five partitions, each of size 20 blocks.
  - Division of memory:
    - The first partition occupies 20 blocks of memory
    - 1 block is used for input, and 1 block each for buffering the other 4 partitions.
- <i>teaches </i>is similarly partitioned into five partitions each of size 80
  - the first is used right away for probing, instead of being written out
- Cost of 3(80 + 320) + 20 +80 = 1300 block transfers for hybrid hash join, instead of 1500 with plain hash-join.
- Hybrid hash-join most useful if <i>M</i> &gt;&gt; 
<p><i>s</i></p>
<p><i>b</i></p>


---


{{HIDDEN}}

### Complex Joins

- Join with a conjunctive condition:
<p><i>r </i>⨝ θ1∧ θ 2∧... ∧ θ<i> n</i><i> s</i></p>
  - Either use nested loops/block nested loops, or</p>
  - Compute the result of one of the simpler joins <i>r </i>⨝ θ<i>i</i><i> s</i></p>
    - final result comprises those tuples in the intermediate result that satisfy the remaining conditions
<p>θ1∧ . . . ∧ θ<i>i</i> –1 ∧ θ<i>i</i> +1 ∧ . . . ∧ θ<i>n</i></p>
- Join with a disjunctive condition
<p><i>r </i>⨝ θ1 ∨ θ2 ∨... ∨ θ<i>n</i><i> s</i></p>
  - Either use nested loops/block nested loops, or</p>
  - Compute as the union of the records in individual joins <i>r </i>⨝ θ<i>i</i><i> s:</i></p>
<p>(<i>r </i>⨝ θ1<i> s</i>) ∪ (<i>r </i>⨝ θ2<i> s) </i>∪ . . . ∪ (<i>r </i>⨝ θ<i>n</i><i> s) </i></p>


---


{{HIDDEN}}

### Joins over Spatial Data

- No simple sort order for spatial joins
- Indexed nested loops join with spatial indices
  - R-trees, quad-trees, k-d-B-trees


---


{{HIDDEN}}

### Other Operations

- <b>Duplicate elimination </b>can be implemented via hashing or sorting.
  - On sorting duplicates will come adjacent to each other, and all but one set of duplicates can be deleted
  - <i> Optimization: </i>duplicates can be deleted during run generation as well as at intermediate merge steps in external sort-merge.
  - Hashing is similar – duplicates will come into the same bucket.
- <b>Projection</b>:
  - perform projection on each tuple
  - followed by duplicate elimination


---

### Other Operations : Aggregation

- <b>Aggregation</b> can be implemented in a manner similar to duplicate elimination.
  - <b> Sorting</b> or <b>hashing</b> can be used to bring tuples in the same group together, and then the aggregate functions can be applied on each group.

Note:

Not included
  - Optimization<i>: </i><b>partial aggregation</b>
    - combine tuples in the same group during run generation and intermediate merges, by computing partial aggregate values
    - For count, min, max, sum: keep aggregate values on tuples found so far in the group
  - When combining partial aggregate for count, add up the partial aggregates
    - For avg, keep sum and count, and divide sum by count at the end


---

{{HIDDEN}}

### Other Operations : Set Operations

- <b>Set operations </b>(∪, ∩ and ):  can either use variant of merge-join after sorting, or variant of hash-join.
- E.g., Set operations using hashing:
<p>1. Partition both relations using the same hash function</p>
<p>2. Process each partition <i>i</i> as follows.  </p>
<p>1. Using a different hashing function, build an in-memory hash </p>
<p>index on $r_i$.</p>
<p>2. Process si as follows</p>
<p><i> r </i>∪<i> s</i>:  </p>
<p>1. Add tuples in $s_i$ to the hash index if they are not already in it.
<p>2. At end of $s_i$ add the tuples in the hash index to the result.


---


{{HIDDEN}}

### Other Operations : Set Operations

- E.g., Set operations using hashing:
<p>1. as before partition $r$ and <i>s, </i></p>
<p>2. as before, process each partition <i>i</i> as follows</p>
<p>1. build a hash index on $r_i$</p>
<p>2. Process si as follows</p>
  - <i> r</i> ∩<i> s</i>: 
<p>1. output tuples in $s_i$ to the result if they are already there in the hash index
  - $r$ –<i> s:</i>
<p>1. for each tuple in $s_i$, if it is there in the hash index, delete it from the index.
<p>2. At end of $s_i$ add remaining tuples in the hash index to the result.


---


{{HIDDEN}}

### Answering Keyword Queries

- Indices mapping keywords to documents
  - For each keyword, store sorted list of document IDs that contain the keyword
    - Commonly referred to as a <b>inverted index</b>
    - E.g.: database:  d1, d4, d11, d45, d77, d123
<p>distributed:  d4, d8, d11, d56, d77, d121, d333</p>
  - To answer a query with several keywords, compute intersection of lists corresponding to those keywords
- To support ranking, inverted lists store extra information
  - “<b>Term frequency</b>” of the keyword in the document
  - “<b>Inverse document frequency</b>” of the keyword
  - <b>Page rank </b>of the document/web page


---


{{HIDDEN}}

### Other Operations : Outer Join

- <b>Outer join </b>can be computed either as 
  - A join followed by addition of null-padded non-participating tuples.</p>
  - by modifying the join algorithms.</p>
- Modifying merge join to compute <i>r </i>⟕<i> s</i>
  - In <i>r </i>⟕<i> s</i>, non participating tuples are those in <i>r </i>– Π$r$(<i>r </i>⨝<i> s</i>)</p>
  - Modify merge-join to compute <i>r </i>⟕<i> s:  </i></p>
    - During merging, for every tuple $t_r$ from <i>r </i>that do not match any tuple in <i>s, </i>output $t_r$ padded with nulls.
  - Right outer-join and full outer-join can be computed similarly.


---


{{HIDDEN}}

### Other Operations : Outer Join

- Modifying hash join to compute <i>r </i>⟕<i> s</i>
  - If  $r$ is probe relation, output non-matching $r$ tuples padded with nulls
  - If $r$ is build relation, when probing keep track of which $r$ tuples matched $s$ tuples.  At end of<i> s</i><i>i</i> output non-matched $r$ tuples padded with nulls


---

{{HIDDEN}}


### Evaluation of Expressions

- So far: we have seen algorithms for individual operations
- Alternatives for evaluating an entire expression tree
  - <b> Materialization</b>:  generate results of an expression whose inputs are relations or are already computed, **materialize** (store) it on disk.  Repeat.
  - **Pipelining**  pass on tuples to parent operations even as an operation is being executed
- We study above alternatives in more detail


---


{{HIDDEN}}

### Materialization

- <b>Materialized evaluation</b>: evaluate one operation at a time, starting at the lowest-level.  Use intermediate results materialized into temporary relations to evaluate next-level operations.
- E.g., in figure below, compute and store then compute the store its join with _instructor_, and finally compute the projection on _name_
<p>)</p>
<p>(</p>
<p>"</p>
<p>Watson</p>
<p>"</p>
<p><i>department</i></p>
<p><i>building</i>=</p>
<p>σ</p>


---


{{HIDDEN}}

### Materialization (Cont.)

- Materialized evaluation is always applicable
- Cost of writing results to disk and reading them back can be quite high
  - Our cost formulas for operations ignore cost of writing results to disk, so
    - Overall cost  =  Sum of costs of individual operations + cost of writing intermediate results to disk
- **Double buffering**: use two output buffers for each operation, when one is full write it to disk while the other is getting filled
  - Allows overlap of disk writes with computation and reduces execution time


---


{{HIDDEN}}

### Pipelining

- **Pipelined evaluation**:  evaluate several operations simultaneously, passing the results of one operation on to the next.
- E.g., in previous expression tree, don’t store result of
  - instead, pass tuples directly to the join..  Similarly, don’t store result of join, pass tuples directly to projection. 
- Much cheaper than materialization: no need to store a temporary relation to disk
- Pipelining may not always be possible – e.g., sort, hash-join. 
- For pipelining to be effective, use evaluation algorithms that generate output tuples even as tuples are received for inputs to the operation. 
- Pipelines can be executed in two ways:  <b>demand driven</b> and **producer driven**
<p>)</p>
<p>(</p>
<p>"</p>
<p>Watson</p>
<p>"</p>
<p><i>department</i></p>
<p><i>building</i>=</p>
<p>σ</p>


---


{{HIDDEN}}

### Pipelining (Cont.)

- In <b>demand driven </b>or <b>lazy</b> evaluation
  - system repeatedly requests next tuple  from top level operation
  - Each operation requests  next tuple from children operations as required, in order to output its next tuple
  - In between calls, operation has to maintain “<b>state</b>” so it knows what to return next
- In <b>producer-driven</b> or <b>eager</b> pipelining
  - Operators produce tuples eagerly and pass them up to their parents
    - Buffer maintained between operators, child puts tuples in buffer, parent removes tuples from buffer<
    - if buffer is full, child waits till there is space in the buffer, and then generates more tuples<
  - System schedules operations that have space in output buffer and can process more input tuples<
- Alternative name: <b>pull</b> and <b>push</b> models of pipelining


---


{{HIDDEN}}

### Pipelining (Cont.)

- Implementation of demand-driven pipelining
  - Each operation is implemented as an <b>iterator</b> implementing the following operations
    - <b> open()</b>
  - E.g. file scan: initialize file scan</p>
    - state: pointer to beginning of file
  - E.g.merge join: sort relations;</p>
    - state: pointers to beginning of sorted relations
    - <b> next()</b>
  - E.g. for file scan: Output next tuple, and advance and store file pointer
  - E.g. for merge join:  continue with merge from earlier state till next output tuple is found.  Save pointers as iterator state.
    - <b> close()</b>


---


{{HIDDEN}}

### Blocking Operations

- <b>Blocking operations</b>:  cannot generate any output until all input is consumed
  - E.g. sorting, aggregation, …
- But can often consume inputs from a pipeline, or produce outputs to a pipeline
- Key idea: blocking operations often have two suboperations
  - E.g. for sort:  run generation and merge
  - For hash join:  partitioning and build-probe 
- Treat them as separate operations
<img src="ă.jpg"/>


---


{{HIDDEN}}

### Pipeline Stages

- <b>Pipeline stages</b>: 
  - All operations in a stage run concurrently
  - A stage can start only after preceding stages have completed execution
<img src="ć.jpg"/>


---


{{HIDDEN}}

### Evaluation Algorithms for Pipelining

- Some algorithms are not able to output results even as they get input tuples
  - E.g. merge join, or hash join
  - intermediate results written to disk and then read back
- Algorithm variants to generate (at least some) results on the fly, as input tuples are read in
  - E.g. hybrid hash join generates output tuples even as probe relation tuples in the in-memory partition (partition 0) are read in
  - <b>Double-pipelined join technique</b>: Hybrid hash join, modified to buffer partition 0 tuples of both relations in-memory, reading them as they become available, and output results of any matches between partition 0 tuples
    - When a new r0 tuple is found, match it with existing s0 tuples, output matches, and save it in r0
    - Symmetrically for s0 tuples


---


{{HIDDEN}}

### Pipeling for Continuous Stream Data

- **Data streams**
  - Data entering database in a continuous manner
  - E.g.  Sensor networks, user clicks, …
- <b>Continuous queries</b>
  - Results get updated as streaming data enters the database
  - Aggregation on windows is often used
    - E.g. <b>tumbling windows </b>divide time into units, e.g. hours, minutes
- Need to use pipelined processing algorithms
  - <b>Punctuations</b> used to infer when all data for a window has been received


---


{{HIDDEN}}

### Query Processing in Memory

- Query compilation to machine code
  - Overheads of interpretation
    - E.g. repeatedly finding attribute location within tuple, from metadata 
    - Overhead of expression evaluation
  - Compilation can avoid many such overheads and speed up query processing
  - Often via generation of Java byte code / LLVM, with just-in-time (JIT) compilation
- Column-oriented storage
  - Allows vector operations (in conjunction with compilation)
- Cache conscious algorithms


---


{{HIDDEN}}

### Cache Conscious Algorithms

- Goal: minimize cache misses, make best use of data fetched into the cache as part of a cache line
- For sorting:
  - Use runs that are as large as L3 cache (a few megabytes) to avoid cache misses during sorting of a run
  - Then merge runs as usual in merge-sort
- For hash-join
  - First create partitions such that build+probe partitions fit in memory
  - Then subpartition further s.t. build subpartition+index fits in L3 cache
    - Speeds up probe phase significantly by avoiding cache misses
- Lay out attributes of tuples to maximize cache usage
  - Attributes that are often accessed together should be stored adjacent to each other
- Use multiple threads for parallel query processing
  - Cache misss leads to stall of one thread, but others can proceed

---

### Epilogue 

- Database performance requires using the correct algorithm
- Largest cost is I/O; At a high level CPU can be ignored
- Selection can scan, use indices, involve comparisons
  - Conjunction, disjunction, and negation all play a role
- Sorting complicated
  - quick-sort only used on the smallest of relations
  - External merge-sort key to limiting I/O
- Joins: blocks/nested/indexed loops
  - Hashes used for equijoins
- Computing costs; choosing well
